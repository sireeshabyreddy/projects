{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd013952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "\n",
    "# Load train data\n",
    "train_data = pd.read_csv('/home/rgukt/Downloads/NER-datasets-dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/train.txt', sep='\\t', header=None, names=['Text'])\n",
    "# You might need to adjust the separator or header as per your file structure\n",
    "\n",
    "# Load test data\n",
    "test_data = pd.read_csv('/home/rgukt/Downloads/NER-datasets-dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/test.txt', sep='\\t', header=None, names=['Text'])\n",
    "\n",
    "# Load validation data\n",
    "valid_data = pd.read_csv('/home/rgukt/Downloads/NER-datasets-dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/valid.txt', sep='\\t', header=None, names=['Text'])\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdfde71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acd5510",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d825f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e49ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061c7afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function to extract words with named entities\n",
    "def extract_named_entities(file_path):\n",
    "    named_entities = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        sentence = []\n",
    "        for line in file:\n",
    "            if line.strip():  # Non-empty line\n",
    "                splits = line.strip().split(' ')\n",
    "                word = splits[0]\n",
    "                entity = splits[-1]\n",
    "                sentence.append([word, entity])\n",
    "            else:  # Empty line denotes end of a sentence\n",
    "                named_entities.append(sentence)\n",
    "                sentence = []\n",
    "        if sentence:  # Append any remaining sentence\n",
    "            named_entities.append(sentence)\n",
    "    return named_entities\n",
    "\n",
    "# Path to the 'data' folder containing train.txt, valid.txt, test.txt\n",
    "data_folder = '/home/rgukt/Downloads/NER-datasets-dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003'\n",
    "\n",
    "train_named_entities = extract_named_entities(os.path.join(data_folder, 'train.txt'))\n",
    "valid_named_entities = extract_named_entities(os.path.join(data_folder, 'valid.txt'))\n",
    "test_named_entities = extract_named_entities(os.path.join(data_folder, 'test.txt'))\n",
    "\n",
    "# Displaying the extracted words along with their named entities for the first sentence in train data\n",
    "print(train_named_entities[0])\n",
    "\n",
    "\n",
    "# Displaying the extracted words along with their named entities for the first sentence in train data\n",
    "print(test_named_entities[9])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb4efd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af693cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "422/422 [==============================] - 46s 92ms/step - loss: 0.6162 - accuracy: 0.8484 - val_loss: 0.2981 - val_accuracy: 0.9115\n",
      "Epoch 2/4\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 0.1977 - accuracy: 0.9398 - val_loss: 0.2155 - val_accuracy: 0.9416\n",
      "Epoch 3/4\n",
      "422/422 [==============================] - 36s 85ms/step - loss: 0.0800 - accuracy: 0.9798 - val_loss: 0.1913 - val_accuracy: 0.9546\n",
      "Epoch 4/4\n",
      "422/422 [==============================] - 36s 86ms/step - loss: 0.0411 - accuracy: 0.9905 - val_loss: 0.1807 - val_accuracy: 0.9569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgukt/.local/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 3s 24ms/step - loss: 0.3031 - accuracy: 0.9308\n",
      "Loss: 0.30310767889022827, Accuracy: 0.9308136701583862\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np \n",
    "# Sample function to load and process CoNLL-03 data\n",
    "def load_conll_data(file_path):\n",
    "    sentences = []  # List to store sentences\n",
    "    ner_tags = []   # List to store corresponding NER tags\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        sentence = []\n",
    "        tags = []\n",
    "        for line in file:\n",
    "            if line != '\\n':  # Non-empty line\n",
    "                word, pos, chunk, tag = line.strip().split()  # Assuming the CoNLL-03 format: word POS-tag Chunk-tag NER-tag\n",
    "                sentence.append(word)\n",
    "                tags.append(tag)\n",
    "            else:  # Empty line indicates the end of a sentence\n",
    "                sentences.append(sentence)\n",
    "                ner_tags.append(tags)\n",
    "                sentence = []\n",
    "                tags = []\n",
    "        \n",
    "        # If the file doesn't end with an empty line\n",
    "        if sentence:\n",
    "            sentences.append(sentence)\n",
    "            ner_tags.append(tags)\n",
    "\n",
    "    return sentences, ner_tags\n",
    "\n",
    "# Replace this with code to load and preprocess CoNLL-03 dataset\n",
    "# You'll need to load sentences and their corresponding NER tags\n",
    "\n",
    "# Example code (this might need modification based on the dataset format)\n",
    "X_train, y_train = load_conll_data(\"/home/rgukt/Downloads/NER-datasets-dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/train.txt\")\n",
    "    \n",
    "X_test, y_test = load_conll_data(\"/home/rgukt/Downloads/NER-datasets-dcb6c7439a7de43abc2448bad5b1d81a47f26c0d/CONLL2003/test.txt\") \n",
    "\n",
    "# Convert words to indices\n",
    "words = set(word for sentence in X_train for word in sentence)\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx['PAD'] = 0\n",
    "word2idx['UNK'] = 1\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "# Convert NER tags to indices\n",
    "tags = set(tag for tags in y_train for tag in tags)\n",
    "tag2idx = {t: i + 1 for i, t in enumerate(tags)}\n",
    "tag2idx['PAD'] = 0\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "max_len = 50  # Define maximum sequence length\n",
    "\n",
    "def convert_data_to_index(X, y, data2idx, label2idx, max_len):\n",
    "    X_idx = [[data2idx.get(w, data2idx['UNK']) for w in s] for s in X]\n",
    "    y_idx = [[label2idx[t] for t in s] for s in y]\n",
    "    X_padded = pad_sequences(X_idx, maxlen=max_len, padding='post')\n",
    "    y_padded = pad_sequences(y_idx, maxlen=max_len, padding='post')\n",
    "    y_categorical = [to_categorical(i, num_classes=len(label2idx)) for i in y_padded]\n",
    "    return X_padded, y_categorical\n",
    "\n",
    "X_train_idx, y_train_cat = convert_data_to_index(X_train, y_train, word2idx, tag2idx, max_len)\n",
    "X_test_idx, y_test_cat = convert_data_to_index(X_test, y_test, word2idx, tag2idx, max_len)\n",
    "\n",
    "# Define the NER model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word2idx), 50, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(units=100, return_sequences=True)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(TimeDistributed(Dense(len(tag2idx), activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_idx, np.array(y_train_cat), batch_size=32, epochs=4, validation_split=0.1)\n",
    "model.save('ner_model.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_idx, np.array(y_test_cat))\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28b2c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3527bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d783a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2230e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 5s 48ms/step - loss: 1.9377 - accuracy: 0.6158\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 2s 47ms/step - loss: 1.0719 - accuracy: 0.6653\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 2s 47ms/step - loss: 0.9538 - accuracy: 0.6883\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 1s 44ms/step - loss: 0.9038 - accuracy: 0.7033\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 0.8811 - accuracy: 0.7048\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 0.8643 - accuracy: 0.7067\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 1s 43ms/step - loss: 0.8483 - accuracy: 0.7101\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 0.8261 - accuracy: 0.7297\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 0.7914 - accuracy: 0.7542\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 0.7347 - accuracy: 0.7826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f1756ef5190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#task2\n",
    "from conllu import parse\n",
    "import tensorflow as tf\n",
    "\n",
    "# Sample function to load and process CoNLL-U data for POS tagging\n",
    "def load_conllu_data(file_path):\n",
    "    sentences = []  # List to store sentences\n",
    "    pos_tags = []   # List to store corresponding POS tags\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "        parsed_data = parse(data)\n",
    "\n",
    "        for sentence in parsed_data:\n",
    "            words = []\n",
    "            tags = []\n",
    "            for token in sentence:\n",
    "                words.append(token['form'])\n",
    "                tags.append(token['upos'])\n",
    "\n",
    "            sentences.append(words)\n",
    "            pos_tags.append(tags)\n",
    "\n",
    "    return sentences, pos_tags\n",
    "\n",
    "# Load and preprocess the English PUD dataset\n",
    "X, y = load_conllu_data(\"/home/rgukt/Downloads/Universal Dependencies 2.12/ud-treebanks-v2.12/UD_English-PUD/en_pud-ud-test.conllu\")\n",
    "\n",
    "# Convert words to indices\n",
    "word_set = set(word for sentence in X for word in sentence)\n",
    "word2idx = {word: idx + 1 for idx, word in enumerate(word_set)}\n",
    "word2idx['PAD'] = 0\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "# Convert POS tags to indices\n",
    "pos_set = set(tag for tags in y for tag in tags)\n",
    "pos2idx = {tag: idx for idx, tag in enumerate(pos_set)}\n",
    "\n",
    "# Convert sentences and POS tags to indices\n",
    "X_idx = [[word2idx[word] for word in sentence] for sentence in X]\n",
    "y_idx = [[pos2idx[tag] for tag in tags] for tags in y]\n",
    "\n",
    "# Define the POS tagging model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(len(word2idx), 32),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(pos2idx), activation='softmax'))\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Padding sequences\n",
    "X_padded = tf.keras.preprocessing.sequence.pad_sequences(X_idx, padding='post')\n",
    "y_padded = tf.keras.preprocessing.sequence.pad_sequences(y_idx, padding='post')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_padded, y_padded, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d730558d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1385a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 2s 21ms/step - loss: 0.6942 - accuracy: 0.8114\n",
      "Test Loss: 0.6941821575164795, Test Accuracy: 0.8113728761672974\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the test data using the same approach as training data\n",
    "X_test, y_test = load_conllu_data(\"/home/rgukt/Downloads/Universal Dependencies 2.12/ud-treebanks-v2.12/UD_English-PUD/en_pud-ud-test.conllu\")\n",
    "# Convert the test data to indices\n",
    "X_test_idx = [[word2idx.get(word, 0) for word in sentence] for sentence in X_test]\n",
    "y_test_idx = [[pos2idx[tag] for tag in tags] for tags in y_test]\n",
    "\n",
    "# Padding sequences for the test data\n",
    "X_test_padded = tf.keras.preprocessing.sequence.pad_sequences(X_test_idx, padding='post')\n",
    "y_test_padded = tf.keras.preprocessing.sequence.pad_sequences(y_test_idx, padding='post')\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test_padded)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387a433c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
